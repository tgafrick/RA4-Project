{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\tgafrick\\Documents\\Github\\RA4-Project\\venv\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import os\n",
    "import nrrd\n",
    "import pytorch_lightning as pl\n",
    "from monai.data import Dataset, DataLoader\n",
    "from monai.transforms import Compose, Lambda, RandSpatialCrop, RandRotate90, ToTensor\n",
    "from monai.networks.nets import UNet\n",
    "#from monai.losses import DiceLoss\n",
    "import tensorflow as tf\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "class SlicePlotter:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def plot_slices(self, original_slice, reconstruction_slice, title):\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.imshow(original_slice, cmap='gray')\n",
    "        plt.title(f'Original {title} Slice')\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.imshow(reconstruction_slice, cmap='gray')\n",
    "        plt.title(f'Reconstructed {title} Slice')\n",
    "        plt.show()\n",
    "\n",
    "class VolumePlotter:\n",
    "    def __init__(self):\n",
    "        self.color_map = np.array([\n",
    "            [0, 0, 0, 0],      # Transparent for background (0)\n",
    "            [255, 0, 0, 255],  # Red for arteries (1)\n",
    "            [0, 0, 255, 255]   # Blue for veins (2)\n",
    "        ], dtype=np.uint8)\n",
    "        self.downsample_factor = 2\n",
    "\n",
    "    def plot_volumes(self, original_volume, reconstruction_volume, title):\n",
    "        # Downsample original volume\n",
    "        original_volume_downsampled = original_volume[::self.downsample_factor, \n",
    "                                                       ::self.downsample_factor, \n",
    "                                                       ::self.downsample_factor]\n",
    "\n",
    "        # Downsample reconstructed volume\n",
    "        reconstruction_volume_downsampled = reconstruction_volume[::self.downsample_factor, \n",
    "                                                                   ::self.downsample_factor, \n",
    "                                                                   ::self.downsample_factor]\n",
    "\n",
    "        # Downsample color map along the first axis to match the number of unique values in the downsampled volume\n",
    "        color_map_downsampled = self.color_map[:len(np.unique(original_volume_downsampled))]\n",
    "\n",
    "        # Plot original and reconstructed volumes with titles using Matplotlib\n",
    "        fig = plt.figure(figsize=(10, 5))\n",
    "\n",
    "        # Plot original volume with assigned colors and no edge color\n",
    "        ax1 = fig.add_subplot(121, projection='3d')\n",
    "        ax1.set_title(f'Original {title} Volume')\n",
    "\n",
    "        # Define colors for each voxel value in the original volume using the downsampled color map\n",
    "        original_colors = np.take(color_map_downsampled, original_volume_downsampled, axis=0) / 255.0\n",
    "        ax1.voxels(original_volume_downsampled, facecolors=original_colors)\n",
    "\n",
    "        # Plot reconstructed volume with assigned colors and no edge color\n",
    "        ax2 = fig.add_subplot(122, projection='3d')\n",
    "        ax2.set_title(f'Reconstructed {title} Volume')\n",
    "\n",
    "        # Define colors for each voxel value in the reconstructed volume using the downsampled color map\n",
    "        reconstruction_colors = np.take(color_map_downsampled, reconstruction_volume_downsampled, axis=0) / 255.0\n",
    "        ax2.voxels(reconstruction_volume_downsampled, facecolors=reconstruction_colors)\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "class AutoEncoder(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.unet = UNet(\n",
    "            spatial_dims=3,\n",
    "            in_channels=1,\n",
    "            out_channels=3,\n",
    "            channels=(16, 32, 64, 128, 64),\n",
    "            strides=(2, 2, 2, 2),\n",
    "            num_res_units=2,\n",
    "        )\n",
    "        class_weights = torch.tensor([0.2, 0.4, 0.4])\n",
    "        self.loss_function = CrossEntropyLoss(weight=class_weights)\n",
    "        self.volume_plotter = VolumePlotter()  # Instantiate VolumePlotter\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.unet(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        x_hat = self.unet(x)\n",
    "        loss = self.loss_function(x_hat, y.squeeze(1).long())\n",
    "        self.log('train_loss', loss)\n",
    "\n",
    "        if self.current_epoch % 10 == 0 and batch_idx == 0:\n",
    "            original_volume = x[0, 0, :, :, :].detach().cpu().numpy().astype(int)  \n",
    "            reconstruction_volume = np.argmax(x_hat[0,:,:,:,:], axis=0)\n",
    "            self.volume_plotter.plot_volumes(original_volume, reconstruction_volume, 'Train')\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        x_hat = self.unet(x)\n",
    "        loss = self.loss_function(x_hat, y.squeeze(1).long())\n",
    "        self.log('val_loss', loss, prog_bar=True)\n",
    "\n",
    "        if self.current_epoch % 10 == 0 and batch_idx == 0:\n",
    "            original_volume = x[0, 0, :, :, :].detach().cpu().numpy().astype(int)  \n",
    "            reconstruction_volume = np.argmax(x_hat[0,:,:,:,:], axis=0)\n",
    "            self.volume_plotter.plot_volumes(original_volume, reconstruction_volume, 'Val')\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "\n",
    "\n",
    "# Define dataset and data loaders for both training and validation sets\n",
    "train_data_dir = \"./images/TrainFullLabels70\"\n",
    "val_data_dir = \"./images/ValFullLabels30\"\n",
    "\n",
    "transform = Compose([\n",
    "    Lambda(func=lambda x: np.expand_dims(x, 0) if x.ndim == 3 else x),\n",
    "    RandSpatialCrop((256, 256, 256), random_size=False),\n",
    "    RandRotate90(prob=0.5),\n",
    "    ToTensor(dtype=torch.float32)\n",
    "])\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data_dir, transform=None):\n",
    "        self.data_dir = data_dir\n",
    "        self.transform = transform\n",
    "        self.files = [f for f in os.listdir(data_dir) if f.endswith('.nrrd')]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_path = os.path.join(self.data_dir, self.files[idx])\n",
    "        data, header = nrrd.read(file_path)\n",
    "        if self.transform:\n",
    "            data = self.transform(data)\n",
    "        return data, data\n",
    "\n",
    "train_dataset = MyDataset(train_data_dir, transform)\n",
    "val_dataset = MyDataset(val_data_dir, transform)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=1, shuffle=True, num_workers=0)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=1, shuffle=False, num_workers=0)\n",
    "\n",
    "# Define a custom callback to plot both training and validation losses\n",
    "\n",
    "class PlotLossCallback(pl.Callback):\n",
    "    def __init__(self, log_dir):\n",
    "        super().__init__()\n",
    "        self.log_dir = log_dir\n",
    "\n",
    "    def on_train_start(self, trainer, pl_module):\n",
    "        # Clear previous graphs\n",
    "        tf.compat.v1.reset_default_graph()\n",
    "        self.writer = SummaryWriter(self.log_dir)\n",
    "\n",
    "    def on_train_epoch_end(self, trainer, pl_module, **kwargs):\n",
    "        epoch = trainer.current_epoch\n",
    "        train_loss = trainer.callback_metrics['train_loss'].cpu().item()\n",
    "        val_loss = trainer.callback_metrics['val_loss'].cpu().item()\n",
    "\n",
    "        # Write both training and validation loss to the same graph with epoch as global step\n",
    "        #self.writer.add_scalars('64 Channel Loss', {'training' : train_loss,\n",
    "                                                                #'validation' : val_loss }, global_step=epoch +1 )\n",
    "    def on_train_end(self, trainer, pl_module):\n",
    "        self.writer.close()\n",
    "\n",
    "#early_stop_callback = pl.callbacks.EarlyStopping(monitor='val_loss', patience=80)\n",
    "#getting rid of early stop callback for now \n",
    "# Define Lightning trainer\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=1500,\n",
    "    callbacks=[PlotLossCallback(\"./logs/\")],\n",
    "    profiler='simple')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name          | Type             | Params\n",
      "---------------------------------------------------\n",
      "0 | unet          | UNet             | 2.1 M \n",
      "1 | loss_function | CrossEntropyLoss | 0     \n",
      "---------------------------------------------------\n",
      "2.1 M     Trainable params\n",
      "0         Non-trainable params\n",
      "2.1 M     Total params\n",
      "8.512     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tgafrick\\Documents\\Github\\RA4-Project\\venv\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=19` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tgafrick\\Documents\\Github\\RA4-Project\\venv\\Lib\\site-packages\\pytorch_lightning\\trainer\\call.py:54: Detected KeyboardInterrupt, attempting graceful shutdown...\n"
     ]
    }
   ],
   "source": [
    "# Initialize model and start training\n",
    "model = AutoEncoder()\n",
    "trainer.fit(model, train_dataloader, val_dataloader)\n",
    "#torch.save(model.state_dict(), \"./images/Model/model_64channel.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load in model \n",
    "model = AutoEncoder()\n",
    "model.load_state_dict(torch.load(\"./images/Model/model_1channel.pth\")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2factor downsample 3d image graphing\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "test_dir = \"./images/3dTest\"\n",
    "test_data = MyDataset(test_dir, transform)\n",
    "test_dataloader = DataLoader(test_data, batch_size=1, shuffle=False)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# Define the color map\n",
    "color_map = np.array([\n",
    "    [0, 0, 0, 0],      # Transparent for background (0)\n",
    "    [255, 0, 0, 255],  # Red for arteries (1)\n",
    "    [0, 0, 255, 255]   # Blue for veins (2)\n",
    "], dtype=np.uint8)\n",
    "\n",
    "# Downsampling factor\n",
    "downsample_factor = 2\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (x, _) in enumerate(test_dataloader):\n",
    "        x_hat = model(x)\n",
    "        \n",
    "        # Reconstruct and display the whole volume from the first test image\n",
    "        original_volume = x[0, 0, :, :, :].detach().cpu().numpy().astype(int)  \n",
    "\n",
    "        # Downsample original volume\n",
    "        original_volume_downsampled = original_volume[::downsample_factor, ::downsample_factor, ::downsample_factor]\n",
    "\n",
    "        reconstruction_volume = np.argmax(x_hat[0,:,:,:,:], axis=0)\n",
    "\n",
    "        # Downsample reconstructed volume\n",
    "        reconstruction_volume_downsampled = reconstruction_volume[::downsample_factor, ::downsample_factor, ::downsample_factor]\n",
    "\n",
    "        # Downsample color map along the first axis to match the number of unique values in the downsampled volume\n",
    "        color_map_downsampled = color_map[:len(np.unique(original_volume_downsampled))]\n",
    "\n",
    "        # Plot original and reconstructed volumes with titles using Matplotlib\n",
    "        fig = plt.figure(figsize=(10, 5))\n",
    "\n",
    "        # Plot original volume with assigned colors and no edge color\n",
    "        ax1 = fig.add_subplot(121, projection='3d')\n",
    "        ax1.set_title('Original Test Volume')\n",
    "\n",
    "        # Define colors for each voxel value in the original volume using the downsampled color map\n",
    "        original_colors = np.take(color_map_downsampled, original_volume_downsampled, axis=0) / 255.0\n",
    "        ax1.voxels(original_volume_downsampled, facecolors=original_colors)\n",
    "\n",
    "        # Plot reconstructed volume with assigned colors and no edge color\n",
    "        ax2 = fig.add_subplot(122, projection='3d')\n",
    "        ax2.set_title('Reconstructed Test Volume')\n",
    "\n",
    "        # Define colors for each voxel value in the reconstructed volume using the downsampled color map\n",
    "        reconstruction_colors = np.take(color_map_downsampled, reconstruction_volume_downsampled, axis=0) / 255.0\n",
    "        ax2.voxels(reconstruction_volume_downsampled, facecolors=reconstruction_colors)\n",
    "\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing model\n",
    "\n",
    "test_dir = \"./images/Test\"\n",
    "test_data = MyDataset(test_dir, transform)\n",
    "test_dataloader = DataLoader(test_data, batch_size = 1, shuffle= False)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (x,_) in enumerate(test_dataloader):\n",
    "        x_hat = model(x)\n",
    "        \n",
    "        # Reconstruct and display a single slice from the first test image\n",
    "        original_slice = x[0, 0, :, :, 128].detach().cpu().numpy() \n",
    "        reconstruction_slice = np.argmax(x_hat[0,:,:,:,128], axis=0)\n",
    "        \n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.imshow(original_slice, cmap='gray')\n",
    "        plt.title('Original Test Slice')\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.imshow(reconstruction_slice, cmap='gray')\n",
    "        plt.title('Reconstructed Test Slice')\n",
    "        plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
