{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import os\n",
    "import nrrd\n",
    "import pytorch_lightning as pl\n",
    "from monai.data import Dataset, DataLoader\n",
    "from monai.transforms import Compose, Lambda, RandSpatialCrop, RandRotate90, ToTensor\n",
    "from monai.networks.nets import AutoEncoder\n",
    "#from monai.losses import DiceLoss\n",
    "import tensorflow as tf\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import matplotlib.pyplot as plt\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "import torch.nn as nn\n",
    "import torch.optim.lr_scheduler as lr_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "#Using AutoEncoder instead of UNet to avoid skip connections \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class SlicePlotter:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def plot_slices(self, original_slice, reconstruction_slice, title):\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.imshow(original_slice, cmap='gray')\n",
    "        plt.title(f'Original {title} Slice')\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.imshow(reconstruction_slice, cmap='gray')\n",
    "        plt.title(f'Reconstructed {title} Slice')\n",
    "        plt.show()\n",
    "\n",
    "class VolumePlotter:\n",
    "    def __init__(self):\n",
    "        self.color_map = np.array([\n",
    "            [0, 0, 0, 0],      # Transparent for background (0)\n",
    "            [255, 0, 0, 255],  # Red for arteries (1)\n",
    "            [0, 0, 255, 255]   # Blue for veins (2)\n",
    "        ], dtype=np.uint8)\n",
    "        self.downsample_factor = 2\n",
    "\n",
    "    def plot_volumes(self, original_volume, reconstruction_volume, title):\n",
    "        # Downsample original volume\n",
    "        original_volume_downsampled = original_volume[::self.downsample_factor, \n",
    "                                                       ::self.downsample_factor, \n",
    "                                                       ::self.downsample_factor]\n",
    "\n",
    "        # Downsample reconstructed volume\n",
    "        reconstruction_volume_downsampled = reconstruction_volume[::self.downsample_factor, \n",
    "                                                                   ::self.downsample_factor, \n",
    "                                                                   ::self.downsample_factor]\n",
    "\n",
    "        # Downsample color map along the first axis to match the number of unique values in the downsampled volume\n",
    "        color_map_downsampled = self.color_map[:len(np.unique(original_volume_downsampled))]\n",
    "\n",
    "        # Plot original and reconstructed volumes with titles using Matplotlib\n",
    "        fig = plt.figure(figsize=(10, 5))\n",
    "\n",
    "        # Plot original volume with assigned colors and no edge color\n",
    "        ax1 = fig.add_subplot(121, projection='3d')\n",
    "        ax1.set_title(f'Original {title} Volume')\n",
    "\n",
    "        # Define colors for each voxel value in the original volume using the downsampled color map\n",
    "        original_colors = np.take(color_map_downsampled, original_volume_downsampled, axis=0) / 255.0\n",
    "        ax1.voxels(original_volume_downsampled, facecolors=original_colors)\n",
    "\n",
    "        # Plot reconstructed volume with assigned colors and no edge color\n",
    "        ax2 = fig.add_subplot(122, projection='3d')\n",
    "        ax2.set_title(f'Reconstructed {title} Volume')\n",
    "\n",
    "        # Define colors for each voxel value in the reconstructed volume using the downsampled color map\n",
    "        reconstruction_colors = np.take(color_map_downsampled, reconstruction_volume_downsampled, axis=0) / 255.0\n",
    "        ax2.voxels(reconstruction_volume_downsampled, facecolors=reconstruction_colors)\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "class AutoEncoderModel(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.autoencoder = AutoEncoder(\n",
    "            spatial_dims=3, \n",
    "            in_channels=1, \n",
    "            out_channels=3,  # Output channels set to 3\n",
    "            channels=(16, 32, 64, 128, 64), \n",
    "            strides=(2, 2, 2, 2, 2), \n",
    "            kernel_size=3, \n",
    "            num_res_units=2\n",
    "        )\n",
    "        # Define class weights\n",
    "        class_weights = torch.tensor([0.2, 0.4, 0.4])\n",
    "        self.loss_function = nn.CrossEntropyLoss(weight=class_weights)\n",
    "        self.volume_plotter = VolumePlotter()  # Instantiate VolumePlotter\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.autoencoder(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        x_hat = self(x)\n",
    "        loss = self.loss_function(x_hat, y.squeeze(1).long())\n",
    "        self.log('train_loss', loss)\n",
    "\n",
    "        if self.current_epoch % 300 == 0 and batch_idx == 0:\n",
    "            original_volume = x[0, 0, :, :, :].detach().cpu().numpy().astype(int)  \n",
    "            reconstruction_volume = np.argmax(x_hat[0,:,:,:,:].detach().cpu().numpy(), axis=0)\n",
    "            self.volume_plotter.plot_volumes(original_volume, reconstruction_volume, 'Train')\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        x_hat = self(x)\n",
    "        loss = self.loss_function(x_hat, y.squeeze(1).long())\n",
    "        self.log('val_loss', loss, prog_bar=True)\n",
    "\n",
    "        if self.current_epoch % 300 == 0 and batch_idx == 0:\n",
    "            original_volume = x[0, 0, :, :, :].detach().cpu().numpy().astype(int)  \n",
    "            reconstruction_volume = np.argmax(x_hat[0,:,:,:,:].detach().cpu().numpy(), axis=0)\n",
    "            self.volume_plotter.plot_volumes(original_volume, reconstruction_volume, 'Val')\n",
    "            torch.save(model.state_dict(), \"./images/Model/noskips.pth\")\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=100, factor=0.2, verbose=True)\n",
    "        return {'optimizer': optimizer, 'lr_scheduler': {'scheduler': scheduler, 'monitor': 'val_loss', 'interval': 'epoch', 'frequency': 1}}\n",
    "\n",
    "# Define dataset and data loaders for both training and validation sets\n",
    "train_data_dir = \"./images/TrainFullLabels70\"\n",
    "val_data_dir = \"./images/ValFullLabels30\"\n",
    "\n",
    "transform = Compose([\n",
    "    Lambda(func=lambda x: np.expand_dims(x, 0) if x.ndim == 3 else x),\n",
    "    RandSpatialCrop((256, 256, 256), random_size=False),\n",
    "    RandRotate90(prob=0.5),\n",
    "    ToTensor(dtype=torch.float32)\n",
    "])\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data_dir, transform=None):\n",
    "        self.data_dir = data_dir\n",
    "        self.transform = transform\n",
    "        self.files = [f for f in os.listdir(data_dir) if f.endswith('.nrrd')]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_path = os.path.join(self.data_dir, self.files[idx])\n",
    "        data, header = nrrd.read(file_path)\n",
    "        if self.transform:\n",
    "            data = self.transform(data)\n",
    "        return data, data\n",
    "\n",
    "train_dataset = MyDataset(train_data_dir, transform)\n",
    "val_dataset = MyDataset(val_data_dir, transform)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=1, shuffle=True, num_workers=0)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=1, shuffle=False, num_workers=0)\n",
    "\n",
    "# Define a custom callback to plot both training and validation losses\n",
    "\n",
    "class PlotLossCallback(pl.Callback):\n",
    "    def __init__(self, log_dir):\n",
    "        super().__init__()\n",
    "        self.log_dir = log_dir\n",
    "\n",
    "    def on_train_start(self, trainer, pl_module):\n",
    "        # Clear previous graphs\n",
    "        tf.compat.v1.reset_default_graph()\n",
    "        self.writer = SummaryWriter(self.log_dir)\n",
    "\n",
    "    def on_train_epoch_end(self, trainer, pl_module, **kwargs):\n",
    "        epoch = trainer.current_epoch\n",
    "        train_loss = trainer.callback_metrics['train_loss'].cpu().item()\n",
    "        val_loss = trainer.callback_metrics['val_loss'].cpu().item()\n",
    "\n",
    "        # Write both training and validation loss to the same graph with epoch as global step\n",
    "        self.writer.add_scalars('64 NS LR2 Loss', {'training' : train_loss, \n",
    "                                                   'validation' : val_loss }, global_step=epoch +1 )\n",
    "    def on_train_end(self, trainer, pl_module):\n",
    "        self.writer.close()\n",
    "\n",
    "'''\n",
    "# Define ModelCheckpoint callback\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath = '/images/Model',\n",
    "    filename = 'NS264C_LR_{epoch}.pth',\n",
    "    save_last=True,\n",
    "    verbose=True,\n",
    "    every_n_epochs=100\n",
    ")\n",
    "'''\n",
    "\n",
    "#early_stop_callback = pl.callbacks.EarlyStopping(monitor='val_loss', patience=80)\n",
    "#getting rid of early stop callback for now \n",
    "# Define Lightning trainer\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=1500,\n",
    "    callbacks=[PlotLossCallback(\"./logs/\")],\n",
    "    profiler='simple')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name          | Type             | Params\n",
      "---------------------------------------------------\n",
      "0 | autoencoder   | AutoEncoder      | 2.8 M \n",
      "1 | loss_function | CrossEntropyLoss | 0     \n",
      "---------------------------------------------------\n",
      "2.8 M     Trainable params\n",
      "0         Non-trainable params\n",
      "2.8 M     Total params\n",
      "11.296    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1091: 100%|██████████| 18/18 [1:43:40<00:00,  0.00it/s, v_num=28, val_loss=0.0811]\n"
     ]
    }
   ],
   "source": [
    "# Initialize model and start training\n",
    "model = AutoEncoderModel()\n",
    "trainer.fit(model, train_dataloader, val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"./images/Model/noskips1500eLR2_64C.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
